{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d54bf5d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def load_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path, index_col=0)\n",
    "    data.columns = range(1, len(data.columns) + 1)\n",
    "    expression_sums = data.iloc[:,:-1].sum(axis=1)\n",
    "    filtered_data = data[(expression_sums < 2000) & (expression_sums > 0)]\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def prepare_data(data, window_size, train_len, test_size=0.1, val_size=0.1):\n",
    "    all_data = data.values[:, 0:train_len+1]\n",
    "    X, y = sliding_window(all_data, window_size)\n",
    "    X = X.astype(float).reshape(-1, window_size)\n",
    "    y = y.astype(float).reshape(-1, 1)\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    adjusted_val_size = val_size / (1 - test_size)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=adjusted_val_size, random_state=42)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)  \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_weight = nn.Linear(channels, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, length = x.size()\n",
    "\n",
    "        x_reshaped = x.view(batch_size * length, channels)\n",
    "\n",
    "        Q = self.query_weight(x_reshaped)  # [batch_size * length, channels]\n",
    "        K = x_reshaped\n",
    "\n",
    "        # Reshape Q, K for bmm\n",
    "        Q = Q.view(batch_size, length, channels)  # [batch_size, length, channels]\n",
    "        K = K.view(batch_size, channels, length)  # [batch_size, channels, length]\n",
    "\n",
    "        # Attention Scores\n",
    "        attention_scores = torch.bmm(Q, K)  # [batch_size, length, length]\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Apply Attention Weights\n",
    "        V = x.view(batch_size, channels, length)  # [batch_size, channels, length]\n",
    "        output = torch.bmm(V, attention_scores.transpose(-2, -1))  # [batch_size, channels, length]\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=2, stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=64, kernel_size=2, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.attention = SelfAttention(64)\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(64, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "\n",
    "        x = self.attention(x)\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "def sliding_window(data, window_size):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for i in range(data.shape[1] - window_size):\n",
    "        features.append(data[:, i:i + window_size])\n",
    "        targets.append(data[:, i + window_size])\n",
    "    return np.array(features), np.array(targets)\n",
    "\n",
    "# Create dataloaders\n",
    "def create_dataloaders(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    X_train_tensor, y_train_tensor = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor, y_val_tensor = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "    train_dataset = GeneExpressionDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = GeneExpressionDataset(X_val_tensor, y_val_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    if X_test is not None and y_test is not None:\n",
    "        X_test_tensor, y_test_tensor = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "        test_dataset = GeneExpressionDataset(X_test_tensor, y_test_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        return train_loader, val_loader, test_loader\n",
    "    else:\n",
    "        return train_loader, val_loader\n",
    "\n",
    "# Train model function\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Test model function\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in test_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            total_loss += loss.item()\n",
    "    average_test_loss = total_loss / len(test_loader)\n",
    "    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n",
    "\n",
    "# Get prediction function\n",
    "def get_prediction(input_data, model, device):\n",
    "    input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "    return prediction.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "def generate_cluster_specific_prediction(predict_nums, train_len, data, window_size, models, device):\n",
    "\n",
    "    gene_symbols = data.index\n",
    "\n",
    "\n",
    "    all_data = data.values[:, :train_len]\n",
    "    predictions = np.zeros((all_data.shape[0], predict_nums))\n",
    "\n",
    "    for cluster, model in models.items():\n",
    "        cluster_indices = np.where(data.iloc[:, -1] == cluster)[0]\n",
    "        cluster_data = all_data[cluster_indices, :]\n",
    "\n",
    "        for i in range(predict_nums):\n",
    "            input_data = cluster_data[:, -window_size:]\n",
    "\n",
    "            pred = get_prediction(input_data, model, device)\n",
    "            predictions[cluster_indices, i] = pred.squeeze()\n",
    "            cluster_data = np.hstack((cluster_data, pred))\n",
    "\n",
    "\n",
    "\n",
    "    # Combine predictions with original data\n",
    "    extended_data = np.concatenate((all_data, predictions), axis=1)\n",
    "    predicted_cols = [\"predicted_\" + str(train_len + i) for i in range(1, predict_nums + 1)]\n",
    "    columns = data.columns.tolist()[:train_len] + predicted_cols\n",
    "    extended_df = pd.DataFrame(extended_data, columns=columns)\n",
    "\n",
    "    # Reattach gene symbols\n",
    "    extended_df.insert(0, 'gene symbol', gene_symbols)\n",
    "\n",
    "    return extended_df\n",
    "\n",
    "def train_models_for_each_cluster(data, window_size, train_len, num_epochs, device):\n",
    "    unique_clusters = np.unique(data.iloc[:, -1])\n",
    "    models = {}\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        print(f\"Training model for cluster {cluster}\")\n",
    "        cluster_data = data[data.iloc[:, -1] == cluster]\n",
    "        X_train, X_val, _, y_train, y_val, _ = prepare_data(cluster_data, window_size, train_len)\n",
    "        loaders = create_dataloaders(X_train, X_val, None, y_train, y_val, None)\n",
    "        train_loader = loaders[0]\n",
    "        val_loader = loaders[1]\n",
    "\n",
    "        model = LinearRegression(3,1).to(device)\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device)\n",
    "        \n",
    "        models[cluster] = model\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703c766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for cluster 1\n",
      "Epoch [1/30], Training Loss: 25.1625, Validation Loss: 31.0282\n",
      "Epoch [2/30], Training Loss: 23.3245, Validation Loss: 29.2856\n",
      "Epoch [3/30], Training Loss: 21.5190, Validation Loss: 27.5848\n",
      "Epoch [4/30], Training Loss: 19.6944, Validation Loss: 25.8975\n",
      "Epoch [5/30], Training Loss: 17.8872, Validation Loss: 24.2405\n",
      "Epoch [6/30], Training Loss: 16.1007, Validation Loss: 22.6058\n",
      "Epoch [7/30], Training Loss: 14.3034, Validation Loss: 20.9643\n",
      "Epoch [8/30], Training Loss: 12.5520, Validation Loss: 19.3826\n",
      "Epoch [9/30], Training Loss: 10.7848, Validation Loss: 17.8065\n",
      "Epoch [10/30], Training Loss: 9.0975, Validation Loss: 16.2902\n",
      "Epoch [11/30], Training Loss: 8.3219, Validation Loss: 14.8547\n",
      "Epoch [12/30], Training Loss: 7.7273, Validation Loss: 13.4885\n",
      "Epoch [13/30], Training Loss: 7.1925, Validation Loss: 12.2191\n",
      "Epoch [14/30], Training Loss: 6.6830, Validation Loss: 11.1065\n",
      "Epoch [15/30], Training Loss: 6.2071, Validation Loss: 10.1365\n",
      "Epoch [16/30], Training Loss: 5.7728, Validation Loss: 9.2365\n",
      "Epoch [17/30], Training Loss: 5.4452, Validation Loss: 8.5118\n",
      "Epoch [18/30], Training Loss: 5.3592, Validation Loss: 7.9053\n",
      "Epoch [19/30], Training Loss: 5.7307, Validation Loss: 7.4750\n",
      "Epoch [20/30], Training Loss: 6.2252, Validation Loss: 7.1679\n",
      "Epoch [21/30], Training Loss: 6.8139, Validation Loss: 6.9494\n",
      "Epoch [22/30], Training Loss: 7.3282, Validation Loss: 6.8131\n",
      "Epoch [23/30], Training Loss: 7.7897, Validation Loss: 6.7482\n",
      "Epoch [24/30], Training Loss: 8.1444, Validation Loss: 6.7208\n",
      "Epoch [25/30], Training Loss: 8.4423, Validation Loss: 6.7023\n",
      "Epoch [26/30], Training Loss: 8.6498, Validation Loss: 6.6902\n",
      "Epoch [27/30], Training Loss: 8.7822, Validation Loss: 6.6820\n",
      "Epoch [28/30], Training Loss: 8.9104, Validation Loss: 6.6760\n",
      "Epoch [29/30], Training Loss: 9.0019, Validation Loss: 6.6700\n",
      "Epoch [30/30], Training Loss: 9.1155, Validation Loss: 6.6676\n",
      "Training model for cluster 2\n",
      "Epoch [1/30], Training Loss: 69.3774, Validation Loss: 31.6858\n",
      "Epoch [2/30], Training Loss: 67.7543, Validation Loss: 30.9481\n",
      "Epoch [3/30], Training Loss: 66.1418, Validation Loss: 30.2151\n",
      "Epoch [4/30], Training Loss: 64.5289, Validation Loss: 29.4819\n",
      "Epoch [5/30], Training Loss: 62.9568, Validation Loss: 28.7672\n",
      "Epoch [6/30], Training Loss: 61.3281, Validation Loss: 28.0269\n",
      "Epoch [7/30], Training Loss: 59.7444, Validation Loss: 27.3069\n",
      "Epoch [8/30], Training Loss: 58.1449, Validation Loss: 26.5797\n",
      "Epoch [9/30], Training Loss: 56.5585, Validation Loss: 25.8584\n",
      "Epoch [10/30], Training Loss: 54.9458, Validation Loss: 25.1252\n",
      "Epoch [11/30], Training Loss: 53.3577, Validation Loss: 24.4032\n",
      "Epoch [12/30], Training Loss: 51.7487, Validation Loss: 23.6717\n",
      "Epoch [13/30], Training Loss: 50.1488, Validation Loss: 22.9443\n",
      "Epoch [14/30], Training Loss: 48.5510, Validation Loss: 22.2179\n",
      "Epoch [15/30], Training Loss: 46.9642, Validation Loss: 21.4964\n",
      "Epoch [16/30], Training Loss: 45.3578, Validation Loss: 20.7662\n",
      "Epoch [17/30], Training Loss: 43.7548, Validation Loss: 20.0374\n",
      "Epoch [18/30], Training Loss: 42.1641, Validation Loss: 19.3141\n",
      "Epoch [19/30], Training Loss: 40.5586, Validation Loss: 18.5842\n",
      "Epoch [20/30], Training Loss: 38.9492, Validation Loss: 17.8526\n",
      "Epoch [21/30], Training Loss: 37.3608, Validation Loss: 17.1305\n",
      "Epoch [22/30], Training Loss: 35.7558, Validation Loss: 16.4014\n",
      "Epoch [23/30], Training Loss: 34.1512, Validation Loss: 15.6728\n",
      "Epoch [24/30], Training Loss: 32.5571, Validation Loss: 14.9515\n",
      "Epoch [25/30], Training Loss: 30.9684, Validation Loss: 14.2340\n",
      "Epoch [26/30], Training Loss: 29.3786, Validation Loss: 13.5168\n",
      "Epoch [27/30], Training Loss: 27.7837, Validation Loss: 12.7974\n",
      "Epoch [28/30], Training Loss: 26.1984, Validation Loss: 12.0876\n",
      "Epoch [29/30], Training Loss: 24.6274, Validation Loss: 11.3916\n",
      "Epoch [30/30], Training Loss: 23.0311, Validation Loss: 10.6896\n",
      "Training model for cluster 3\n",
      "Epoch [1/30], Training Loss: 10.1639, Validation Loss: 37.3368\n",
      "Epoch [2/30], Training Loss: 10.0309, Validation Loss: 36.8341\n",
      "Epoch [3/30], Training Loss: 9.9012, Validation Loss: 36.3441\n",
      "Epoch [4/30], Training Loss: 9.7676, Validation Loss: 35.8393\n",
      "Epoch [5/30], Training Loss: 9.6363, Validation Loss: 35.3434\n",
      "Epoch [6/30], Training Loss: 9.5024, Validation Loss: 34.8381\n",
      "Epoch [7/30], Training Loss: 9.3694, Validation Loss: 34.3361\n",
      "Epoch [8/30], Training Loss: 9.2393, Validation Loss: 33.8449\n",
      "Epoch [9/30], Training Loss: 9.1083, Validation Loss: 33.3504\n",
      "Epoch [10/30], Training Loss: 8.9757, Validation Loss: 32.8501\n",
      "Epoch [11/30], Training Loss: 8.8442, Validation Loss: 32.3543\n",
      "Epoch [12/30], Training Loss: 8.7130, Validation Loss: 31.8593\n",
      "Epoch [13/30], Training Loss: 8.5817, Validation Loss: 31.3641\n",
      "Epoch [14/30], Training Loss: 8.4484, Validation Loss: 30.8613\n",
      "Epoch [15/30], Training Loss: 8.3164, Validation Loss: 30.3636\n",
      "Epoch [16/30], Training Loss: 8.1860, Validation Loss: 29.8720\n",
      "Epoch [17/30], Training Loss: 8.0552, Validation Loss: 29.3789\n",
      "Epoch [18/30], Training Loss: 7.9229, Validation Loss: 28.8801\n",
      "Epoch [19/30], Training Loss: 7.7903, Validation Loss: 28.3804\n",
      "Epoch [20/30], Training Loss: 7.6584, Validation Loss: 27.8834\n",
      "Epoch [21/30], Training Loss: 7.5268, Validation Loss: 27.3877\n",
      "Epoch [22/30], Training Loss: 7.3958, Validation Loss: 26.8944\n",
      "Epoch [23/30], Training Loss: 7.2619, Validation Loss: 26.3905\n",
      "Epoch [24/30], Training Loss: 7.1328, Validation Loss: 25.9046\n",
      "Epoch [25/30], Training Loss: 6.9988, Validation Loss: 25.4001\n",
      "Epoch [26/30], Training Loss: 6.8684, Validation Loss: 24.9095\n",
      "Epoch [27/30], Training Loss: 6.7358, Validation Loss: 24.4109\n",
      "Epoch [28/30], Training Loss: 6.6046, Validation Loss: 23.9178\n",
      "Epoch [29/30], Training Loss: 6.4711, Validation Loss: 23.4165\n",
      "Epoch [30/30], Training Loss: 6.3395, Validation Loss: 22.9218\n",
      "Training model for cluster 4\n",
      "Epoch [1/30], Training Loss: 9.8028, Validation Loss: 12.3854\n",
      "Epoch [2/30], Training Loss: 9.3891, Validation Loss: 11.7271\n",
      "Epoch [3/30], Training Loss: 8.9691, Validation Loss: 11.0581\n",
      "Epoch [4/30], Training Loss: 8.5540, Validation Loss: 10.3975\n",
      "Epoch [5/30], Training Loss: 8.1338, Validation Loss: 9.7282\n",
      "Epoch [6/30], Training Loss: 7.7235, Validation Loss: 9.0772\n",
      "Epoch [7/30], Training Loss: 7.3144, Validation Loss: 8.4310\n",
      "Epoch [8/30], Training Loss: 6.9144, Validation Loss: 7.8168\n",
      "Epoch [9/30], Training Loss: 6.5165, Validation Loss: 7.2419\n",
      "Epoch [10/30], Training Loss: 6.1229, Validation Loss: 6.6787\n",
      "Epoch [11/30], Training Loss: 5.7354, Validation Loss: 6.1459\n",
      "Epoch [12/30], Training Loss: 5.3752, Validation Loss: 5.6607\n",
      "Epoch [13/30], Training Loss: 5.0215, Validation Loss: 5.1980\n",
      "Epoch [14/30], Training Loss: 4.6946, Validation Loss: 4.7860\n",
      "Epoch [15/30], Training Loss: 4.3903, Validation Loss: 4.4290\n",
      "Epoch [16/30], Training Loss: 4.1157, Validation Loss: 4.1319\n",
      "Epoch [17/30], Training Loss: 3.8715, Validation Loss: 3.8858\n",
      "Epoch [18/30], Training Loss: 3.6605, Validation Loss: 3.6936\n",
      "Epoch [19/30], Training Loss: 3.4684, Validation Loss: 3.5505\n",
      "Epoch [20/30], Training Loss: 3.3091, Validation Loss: 3.4495\n",
      "Epoch [21/30], Training Loss: 3.1901, Validation Loss: 3.3752\n",
      "Epoch [22/30], Training Loss: 3.0859, Validation Loss: 3.3106\n",
      "Epoch [23/30], Training Loss: 2.9949, Validation Loss: 3.2561\n",
      "Epoch [24/30], Training Loss: 2.9356, Validation Loss: 3.2176\n",
      "Epoch [25/30], Training Loss: 2.8862, Validation Loss: 3.1848\n",
      "Epoch [26/30], Training Loss: 2.8431, Validation Loss: 3.1561\n",
      "Epoch [27/30], Training Loss: 2.8138, Validation Loss: 3.1346\n",
      "Epoch [28/30], Training Loss: 2.7906, Validation Loss: 3.1155\n",
      "Epoch [29/30], Training Loss: 2.7916, Validation Loss: 3.1036\n",
      "Epoch [30/30], Training Loss: 2.7826, Validation Loss: 3.0888\n",
      "Training model for cluster 5\n",
      "Epoch [1/30], Training Loss: 7.4702, Validation Loss: 31.7068\n",
      "Epoch [2/30], Training Loss: 6.8137, Validation Loss: 30.0634\n",
      "Epoch [3/30], Training Loss: 6.1536, Validation Loss: 28.4114\n",
      "Epoch [4/30], Training Loss: 5.4977, Validation Loss: 26.7792\n",
      "Epoch [5/30], Training Loss: 4.8406, Validation Loss: 25.1493\n",
      "Epoch [6/30], Training Loss: 4.1874, Validation Loss: 23.5312\n",
      "Epoch [7/30], Training Loss: 3.5326, Validation Loss: 21.9113\n",
      "Epoch [8/30], Training Loss: 2.8831, Validation Loss: 20.3096\n",
      "Epoch [9/30], Training Loss: 2.2321, Validation Loss: 18.7234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Training Loss: 1.5946, Validation Loss: 17.1880\n",
      "Epoch [11/30], Training Loss: 0.9648, Validation Loss: 15.6872\n",
      "Epoch [12/30], Training Loss: 0.3485, Validation Loss: 14.2645\n",
      "Epoch [13/30], Training Loss: 0.7895, Validation Loss: 12.9632\n",
      "Epoch [14/30], Training Loss: 1.3196, Validation Loss: 11.7651\n",
      "Epoch [15/30], Training Loss: 1.8322, Validation Loss: 10.6591\n",
      "Epoch [16/30], Training Loss: 2.3123, Validation Loss: 9.6827\n",
      "Epoch [17/30], Training Loss: 2.7788, Validation Loss: 8.8315\n",
      "Epoch [18/30], Training Loss: 3.2007, Validation Loss: 8.1275\n",
      "Epoch [19/30], Training Loss: 3.5920, Validation Loss: 7.5319\n",
      "Epoch [20/30], Training Loss: 3.9402, Validation Loss: 7.0757\n",
      "Epoch [21/30], Training Loss: 4.2508, Validation Loss: 6.7259\n",
      "Epoch [22/30], Training Loss: 4.5292, Validation Loss: 6.4815\n",
      "Epoch [23/30], Training Loss: 4.7647, Validation Loss: 6.3135\n",
      "Epoch [24/30], Training Loss: 4.9432, Validation Loss: 6.2243\n",
      "Epoch [25/30], Training Loss: 5.0787, Validation Loss: 6.1688\n",
      "Epoch [26/30], Training Loss: 5.1775, Validation Loss: 6.1293\n",
      "Epoch [27/30], Training Loss: 5.2513, Validation Loss: 6.0963\n",
      "Epoch [28/30], Training Loss: 5.3155, Validation Loss: 6.0719\n",
      "Epoch [29/30], Training Loss: 5.3394, Validation Loss: 6.0533\n",
      "Epoch [30/30], Training Loss: 5.3525, Validation Loss: 6.0375\n",
      "Training model for cluster 6\n",
      "Epoch [1/30], Training Loss: 3.7293, Validation Loss: 9.9340\n",
      "Epoch [2/30], Training Loss: 3.4467, Validation Loss: 9.1311\n",
      "Epoch [3/30], Training Loss: 3.1666, Validation Loss: 8.3452\n",
      "Epoch [4/30], Training Loss: 2.8944, Validation Loss: 7.5912\n",
      "Epoch [5/30], Training Loss: 2.6296, Validation Loss: 6.8872\n",
      "Epoch [6/30], Training Loss: 2.3969, Validation Loss: 6.3057\n",
      "Epoch [7/30], Training Loss: 2.1958, Validation Loss: 5.8305\n",
      "Epoch [8/30], Training Loss: 2.0787, Validation Loss: 5.4808\n",
      "Epoch [9/30], Training Loss: 2.0502, Validation Loss: 5.2247\n",
      "Epoch [10/30], Training Loss: 2.0264, Validation Loss: 5.0404\n",
      "Epoch [11/30], Training Loss: 2.0056, Validation Loss: 4.9149\n",
      "Epoch [12/30], Training Loss: 1.9880, Validation Loss: 4.8360\n",
      "Epoch [13/30], Training Loss: 1.9728, Validation Loss: 4.7817\n",
      "Epoch [14/30], Training Loss: 1.9584, Validation Loss: 4.7430\n",
      "Epoch [15/30], Training Loss: 1.9467, Validation Loss: 4.7176\n",
      "Epoch [16/30], Training Loss: 1.9350, Validation Loss: 4.6971\n",
      "Epoch [17/30], Training Loss: 1.9261, Validation Loss: 4.6844\n",
      "Epoch [18/30], Training Loss: 1.9167, Validation Loss: 4.6714\n",
      "Epoch [19/30], Training Loss: 1.9078, Validation Loss: 4.6598\n",
      "Epoch [20/30], Training Loss: 1.8989, Validation Loss: 4.6483\n",
      "Epoch [21/30], Training Loss: 1.8895, Validation Loss: 4.6364\n",
      "Epoch [22/30], Training Loss: 1.8814, Validation Loss: 4.6260\n",
      "Epoch [23/30], Training Loss: 1.8722, Validation Loss: 4.6146\n",
      "Epoch [24/30], Training Loss: 1.8643, Validation Loss: 4.6043\n",
      "Epoch [25/30], Training Loss: 1.8577, Validation Loss: 4.5959\n",
      "Epoch [26/30], Training Loss: 1.8486, Validation Loss: 4.5847\n",
      "Epoch [27/30], Training Loss: 1.8404, Validation Loss: 4.5746\n",
      "Epoch [28/30], Training Loss: 1.8329, Validation Loss: 4.5652\n",
      "Epoch [29/30], Training Loss: 1.8252, Validation Loss: 4.5557\n",
      "Epoch [30/30], Training Loss: 1.8168, Validation Loss: 4.5454\n",
      "Prediction generation complete.\n"
     ]
    }
   ],
   "source": [
    "data_path = 'ForeBrain_TPM_c6.csv'\n",
    "data = load_preprocess_data(data_path)\n",
    "train_len = 4\n",
    "window_size = 3\n",
    "num_epochs = 30\n",
    "\n",
    "models = train_models_for_each_cluster(data, window_size, train_len, num_epochs, device)\n",
    "results = generate_cluster_specific_prediction(3, train_len, data, window_size, models, device)\n",
    "\n",
    "results.to_csv(f'{data_path[:-4]}_results.csv', index=False)\n",
    "print(\"Prediction generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
