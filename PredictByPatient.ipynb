{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ac6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3db7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P2_d.3_Severe</th>\n",
       "      <th>P2_d.2_Severe</th>\n",
       "      <th>P2_d.1_Severe</th>\n",
       "      <th>P2_d0_Severe</th>\n",
       "      <th>P2_d1_Severe</th>\n",
       "      <th>P2_d2_Severe</th>\n",
       "      <th>P2_d4_Severe</th>\n",
       "      <th>P2_d5_Severe</th>\n",
       "      <th>P2_d6_Severe</th>\n",
       "      <th>P2_d7_Severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.075520</td>\n",
       "      <td>1.026133</td>\n",
       "      <td>0.915501</td>\n",
       "      <td>1.073553</td>\n",
       "      <td>1.071498</td>\n",
       "      <td>1.180263</td>\n",
       "      <td>1.134210</td>\n",
       "      <td>1.023740</td>\n",
       "      <td>0.862322</td>\n",
       "      <td>1.216322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.698694</td>\n",
       "      <td>2.946050</td>\n",
       "      <td>3.070210</td>\n",
       "      <td>2.749033</td>\n",
       "      <td>2.474782</td>\n",
       "      <td>2.521224</td>\n",
       "      <td>3.118423</td>\n",
       "      <td>2.899919</td>\n",
       "      <td>2.228785</td>\n",
       "      <td>2.582810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.215248</td>\n",
       "      <td>16.305909</td>\n",
       "      <td>20.653113</td>\n",
       "      <td>23.851245</td>\n",
       "      <td>19.229484</td>\n",
       "      <td>12.259623</td>\n",
       "      <td>15.147942</td>\n",
       "      <td>17.194324</td>\n",
       "      <td>18.757502</td>\n",
       "      <td>14.604062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.742809</td>\n",
       "      <td>16.794703</td>\n",
       "      <td>16.702408</td>\n",
       "      <td>15.571379</td>\n",
       "      <td>12.912417</td>\n",
       "      <td>11.305714</td>\n",
       "      <td>11.666400</td>\n",
       "      <td>13.257035</td>\n",
       "      <td>12.938406</td>\n",
       "      <td>11.183513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.056643</td>\n",
       "      <td>5.196954</td>\n",
       "      <td>5.927200</td>\n",
       "      <td>4.929898</td>\n",
       "      <td>4.396903</td>\n",
       "      <td>4.778500</td>\n",
       "      <td>4.836971</td>\n",
       "      <td>5.920507</td>\n",
       "      <td>4.977870</td>\n",
       "      <td>5.289075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22544</th>\n",
       "      <td>27.733099</td>\n",
       "      <td>25.671589</td>\n",
       "      <td>23.300585</td>\n",
       "      <td>26.766472</td>\n",
       "      <td>28.679802</td>\n",
       "      <td>23.457545</td>\n",
       "      <td>23.662283</td>\n",
       "      <td>25.612790</td>\n",
       "      <td>28.814464</td>\n",
       "      <td>27.821749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22545</th>\n",
       "      <td>42.585535</td>\n",
       "      <td>40.830882</td>\n",
       "      <td>43.438380</td>\n",
       "      <td>41.474756</td>\n",
       "      <td>39.943498</td>\n",
       "      <td>40.780043</td>\n",
       "      <td>45.020848</td>\n",
       "      <td>37.240145</td>\n",
       "      <td>35.611592</td>\n",
       "      <td>37.554267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22546</th>\n",
       "      <td>34.148099</td>\n",
       "      <td>32.304270</td>\n",
       "      <td>38.698699</td>\n",
       "      <td>40.060142</td>\n",
       "      <td>44.919794</td>\n",
       "      <td>40.766481</td>\n",
       "      <td>33.225917</td>\n",
       "      <td>43.585063</td>\n",
       "      <td>51.328681</td>\n",
       "      <td>44.301609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22547</th>\n",
       "      <td>19.105599</td>\n",
       "      <td>23.988768</td>\n",
       "      <td>31.755251</td>\n",
       "      <td>18.732984</td>\n",
       "      <td>24.067450</td>\n",
       "      <td>24.702550</td>\n",
       "      <td>22.369356</td>\n",
       "      <td>34.429848</td>\n",
       "      <td>17.829453</td>\n",
       "      <td>29.880144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22548</th>\n",
       "      <td>20.349780</td>\n",
       "      <td>36.341788</td>\n",
       "      <td>22.461844</td>\n",
       "      <td>27.111697</td>\n",
       "      <td>26.605127</td>\n",
       "      <td>30.942741</td>\n",
       "      <td>20.474001</td>\n",
       "      <td>38.476743</td>\n",
       "      <td>23.253335</td>\n",
       "      <td>29.187922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22549 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       P2_d.3_Severe  P2_d.2_Severe  P2_d.1_Severe  P2_d0_Severe  \\\n",
       "0           1.075520       1.026133       0.915501      1.073553   \n",
       "1           2.698694       2.946050       3.070210      2.749033   \n",
       "2          22.215248      16.305909      20.653113     23.851245   \n",
       "3          14.742809      16.794703      16.702408     15.571379   \n",
       "4           6.056643       5.196954       5.927200      4.929898   \n",
       "...              ...            ...            ...           ...   \n",
       "22544      27.733099      25.671589      23.300585     26.766472   \n",
       "22545      42.585535      40.830882      43.438380     41.474756   \n",
       "22546      34.148099      32.304270      38.698699     40.060142   \n",
       "22547      19.105599      23.988768      31.755251     18.732984   \n",
       "22548      20.349780      36.341788      22.461844     27.111697   \n",
       "\n",
       "       P2_d1_Severe  P2_d2_Severe  P2_d4_Severe  P2_d5_Severe  P2_d6_Severe  \\\n",
       "0          1.071498      1.180263      1.134210      1.023740      0.862322   \n",
       "1          2.474782      2.521224      3.118423      2.899919      2.228785   \n",
       "2         19.229484     12.259623     15.147942     17.194324     18.757502   \n",
       "3         12.912417     11.305714     11.666400     13.257035     12.938406   \n",
       "4          4.396903      4.778500      4.836971      5.920507      4.977870   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "22544     28.679802     23.457545     23.662283     25.612790     28.814464   \n",
       "22545     39.943498     40.780043     45.020848     37.240145     35.611592   \n",
       "22546     44.919794     40.766481     33.225917     43.585063     51.328681   \n",
       "22547     24.067450     24.702550     22.369356     34.429848     17.829453   \n",
       "22548     26.605127     30.942741     20.474001     38.476743     23.253335   \n",
       "\n",
       "       P2_d7_Severe  \n",
       "0          1.216322  \n",
       "1          2.582810  \n",
       "2         14.604062  \n",
       "3         11.183513  \n",
       "4          5.289075  \n",
       "...             ...  \n",
       "22544     27.821749  \n",
       "22545     37.554267  \n",
       "22546     44.301609  \n",
       "22547     29.880144  \n",
       "22548     29.187922  \n",
       "\n",
       "[22549 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Patient = '2'\n",
    "data = pd.read_csv(f'p{Patient}_tpm.csv')\n",
    "gene_symbols = data['Gene Name'].values.reshape(-1, 1)\n",
    "data.drop(data.columns[0], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cbe949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Training Loss: 11.2538, Validation Loss: 32.6546\n",
      "Epoch [2/30], Training Loss: 9.8539, Validation Loss: 30.6137\n",
      "Epoch [3/30], Training Loss: 8.9694, Validation Loss: 29.3592\n",
      "Epoch [4/30], Training Loss: 8.2827, Validation Loss: 28.4910\n",
      "Epoch [5/30], Training Loss: 7.9790, Validation Loss: 27.7930\n",
      "Epoch [6/30], Training Loss: 7.6939, Validation Loss: 27.2066\n",
      "Epoch [7/30], Training Loss: 7.4113, Validation Loss: 26.7152\n",
      "Epoch [8/30], Training Loss: 7.0727, Validation Loss: 26.2874\n",
      "Epoch [9/30], Training Loss: 6.9292, Validation Loss: 25.8896\n",
      "Epoch [10/30], Training Loss: 6.6751, Validation Loss: 25.5138\n",
      "Epoch [11/30], Training Loss: 6.4811, Validation Loss: 25.2254\n",
      "Epoch [12/30], Training Loss: 6.5186, Validation Loss: 24.8967\n",
      "Epoch [13/30], Training Loss: 6.4402, Validation Loss: 24.5987\n",
      "Epoch [14/30], Training Loss: 6.4408, Validation Loss: 24.3496\n",
      "Epoch [15/30], Training Loss: 6.3709, Validation Loss: 24.1092\n",
      "Epoch [16/30], Training Loss: 6.2879, Validation Loss: 23.8824\n",
      "Epoch [17/30], Training Loss: 6.2340, Validation Loss: 23.6709\n",
      "Epoch [18/30], Training Loss: 6.0949, Validation Loss: 23.4817\n",
      "Epoch [19/30], Training Loss: 6.0686, Validation Loss: 23.3022\n",
      "Epoch [20/30], Training Loss: 5.9718, Validation Loss: 23.1482\n",
      "Epoch [21/30], Training Loss: 5.8692, Validation Loss: 22.9949\n",
      "Epoch [22/30], Training Loss: 5.6624, Validation Loss: 22.8093\n",
      "Epoch [23/30], Training Loss: 5.6705, Validation Loss: 22.6744\n",
      "Epoch [24/30], Training Loss: 5.5757, Validation Loss: 22.5533\n",
      "Epoch [25/30], Training Loss: 5.4563, Validation Loss: 22.4342\n",
      "Epoch [26/30], Training Loss: 5.4037, Validation Loss: 22.2845\n",
      "Epoch [27/30], Training Loss: 5.2487, Validation Loss: 22.1313\n",
      "Epoch [28/30], Training Loss: 5.2086, Validation Loss: 22.0369\n",
      "Epoch [29/30], Training Loss: 5.0927, Validation Loss: 21.8820\n",
      "Epoch [30/30], Training Loss: 5.1277, Validation Loss: 21.8176\n",
      "Average Test Loss: 26.8329\n"
     ]
    }
   ],
   "source": [
    "# Define Dataset\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Adding an RNN layer\n",
    "        self.rnn = nn.RNN(input_size=128, hidden_size=32, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(32, 1)  # Adjusted input dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Adding a dimension for the single channel at each time step\n",
    "        \n",
    "        # Pass input through CNN\n",
    "        cnn_out = self.relu(self.conv1(x))\n",
    "        cnn_out = self.relu(self.conv2(cnn_out))\n",
    "        \n",
    "        # Prepare CNN output for RNN input\n",
    "        rnn_input = cnn_out.permute(0, 2, 1)  # Reorder dimensions to [batch_size, seq_len, num_features]\n",
    "        \n",
    "        # Pass CNN output through RNN\n",
    "        rnn_out, _ = self.rnn(rnn_input)\n",
    "        rnn_out = rnn_out[:, -1, :]  # Take the output from the last time step of the RNN\n",
    "        \n",
    "        # Pass RNN output through FC layer\n",
    "        out = self.fc(rnn_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(in_features=6, out_features=128)  # Assume input size is 6 for simplicity\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc4 = nn.Linear(in_features=32, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "\n",
    "        x = self.relu(self.fc2(x))\n",
    "\n",
    "        x = self.relu(self.fc3(x))\n",
    "\n",
    "        out = self.fc4(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class RNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN1D, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=256, num_layers=1, batch_first=True)  # Single RNN layer\n",
    "        self.fc = nn.Linear(256, 1)  # Adjusted input dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)  # Adding a dimension for the single feature at each time step\n",
    "        \n",
    "        # Pass input through RNN\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        rnn_out = rnn_out[:, -1, :]  # Take the output from the last time step of the RNN\n",
    "        \n",
    "        # Pass RNN output through FC layer\n",
    "        out = self.fc(rnn_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "def sliding_window(data, window_size):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for i in range(data.shape[1]-window_size):\n",
    "        features.append(data[:,i:i+window_size])\n",
    "        targets.append(data[:,i+window_size])\n",
    "    return np.array(features), np.array(targets)\n",
    "\n",
    "# Extract features and targets using sliding window\n",
    "window_size = 6\n",
    "all_data = data.values[:,0:7]\n",
    "X, y = sliding_window(all_data, window_size)\n",
    "X = X.astype(float).reshape(-1, window_size)\n",
    "y = y.astype(float).reshape(-1, 1)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = GeneExpressionDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = GeneExpressionDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = GeneExpressionDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN1D().to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def direction_loss(pred, target):\n",
    "    pred_diff = pred[1:] - pred[:-1]\n",
    "    target_diff = target[1:] - target[:-1]\n",
    "    mask = (pred_diff * target_diff) < 0\n",
    "    return mask.float().mean()\n",
    "\n",
    "direction_weight = 0.5\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_features, batch_targets in train_loader:\n",
    "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        loss_mae = criterion(outputs, batch_targets)\n",
    "        d_loss = direction_loss(outputs, batch_targets)\n",
    "        loss = loss_mae + direction_weight * d_loss\n",
    "#         loss = loss_mae\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in val_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_targets in test_loader:\n",
    "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        total_test_loss += loss.item()\n",
    "        \n",
    "average_test_loss = total_test_loss / len(test_loader)\n",
    "print(f\"Average Test Loss: {average_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f12a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data.iloc[:,1:7].values\n",
    "input_data = input_data.astype(float).reshape(-1, window_size)\n",
    "input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_d7 = model(input_tensor).cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    \n",
    "data = pd.read_csv(f'p{Patient}_tpm.csv')\n",
    "ordered_data = data[['Gene Name', f'P{Patient}_d.3_Severe',f'P{Patient}_d.2_Severe',f'P{Patient}_d.1_Severe', f'P{Patient}_d0_Severe', f'P{Patient}_d1_Severe', f'P{Patient}_d2_Severe', f'P{Patient}_d3_Severe']].values\n",
    "\n",
    "# Concatenate everything: gene symbols, ordered_data, predicted_6th, and predicted_7th\n",
    "extended_data = np.concatenate((ordered_data, predicted_d7), axis=1)\n",
    "\n",
    "# Define columns\n",
    "columns = ['Gene Name', 'd-3','d-2' ,'d-1', 'd0', 'd1', 'd2', 'd3','d4']\n",
    "\n",
    "# Create the DataFrame\n",
    "extended_df = pd.DataFrame(extended_data, columns=columns)\n",
    "    \n",
    "extended_df.to_csv(f'p{Patient}_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d05f56f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, window_size)\n\u001b[0;32m     67\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m X_temp, X_test, y_temp, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     70\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_temp, y_temp, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     72\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2619\u001b[0m )\n\u001b[0;32m   2621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2270\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2277\u001b[0m     )\n\u001b[0;32m   2279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "Patient = '5'\n",
    "data = pd.read_csv(f'p{Patient}_tpm.csv')\n",
    "gene_symbols = data['Gene Name'].values.reshape(-1, 1)\n",
    "\n",
    "data = data.iloc[:, 1:]\n",
    "data.drop(data.columns[1], axis=1, inplace=True)\n",
    "\n",
    "# Define Dataset\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        # CNN Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=32, num_layers=1, batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(64 + 32, 1) # Combined output from CNN and RNN\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through CNN\n",
    "        cnn_out = x.unsqueeze(1)  # Add channel dimension\n",
    "        cnn_out = self.relu(self.conv1(cnn_out))\n",
    "        cnn_out = self.relu(self.conv2(cnn_out))\n",
    "        cnn_out = self.relu(self.conv3(cnn_out))\n",
    "        cnn_out = cnn_out.view(cnn_out.size(0), -1)  # Flatten\n",
    "        \n",
    "        # Pass input through RNN\n",
    "        rnn_out, _ = self.rnn(x.unsqueeze(2))\n",
    "        rnn_out = rnn_out[:, -1, :]  # Take the output from the last RNN unit\n",
    "        \n",
    "        # Concatenate outputs and pass through FC layers\n",
    "        combined_out = torch.cat((cnn_out, rnn_out), dim=1)\n",
    "        out = self.fc1(combined_out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def sliding_window(data, window_size):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for i in range(data.shape[1]-window_size):\n",
    "        features.append(data[:,i:i+window_size])\n",
    "        targets.append(data[:,i+window_size])\n",
    "    return np.array(features), np.array(targets)\n",
    "\n",
    "# Extract features and targets using sliding window\n",
    "window_size = 6\n",
    "all_data = data.values[:,1:9]\n",
    "X, y = sliding_window(all_data, window_size)\n",
    "X = X.astype(float).reshape(-1, window_size)\n",
    "y = y.astype(float).reshape(-1, 1)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = GeneExpressionDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = GeneExpressionDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = GeneExpressionDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN1D().to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def direction_loss(pred, target):\n",
    "    pred_diff = pred[1:] - pred[:-1]\n",
    "    target_diff = target[1:] - target[:-1]\n",
    "    mask = (pred_diff * target_diff) < 0\n",
    "    return mask.float().mean()\n",
    "\n",
    "direction_weight = 0.5\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_features, batch_targets in train_loader:\n",
    "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        loss_mae = criterion(outputs, batch_targets)\n",
    "        d_loss = direction_loss(outputs, batch_targets)\n",
    "        loss = loss_mae + direction_weight * d_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in val_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_targets in test_loader:\n",
    "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        total_test_loss += loss.item()\n",
    "        \n",
    "average_test_loss = total_test_loss / len(test_loader)\n",
    "print(f\"Average Test Loss: {average_test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cc738b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error for p2 d6: 6.8126\n",
      "Mean absolute error for p2 d7: 5.2301\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "predicted_d6_tensor = torch.tensor(predicted_d6, dtype=torch.float32)\n",
    "predicted_d7_tensor = torch.tensor(predicted_d7, dtype=torch.float32)\n",
    "actual_values1_tensor = torch.tensor(data.iloc[:, -2].values.astype(float).reshape(-1, 1), dtype=torch.float32)\n",
    "actual_values_tensor = torch.tensor(data.iloc[:, -1].values.astype(float).reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Compute MAE using PyTorch\n",
    "mae_loss1 = criterion(predicted_d6_tensor, actual_values1_tensor).item()\n",
    "mae_loss = criterion(predicted_d7_tensor, actual_values_tensor).item()\n",
    "\n",
    "print(f'Mean absolute error for p{Patient} d6: {mae_loss1:.4f}')\n",
    "print(f'Mean absolute error for p{Patient} d7: {mae_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
