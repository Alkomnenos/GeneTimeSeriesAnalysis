{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba976eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "840b6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_data(file_path):\n",
    "    data = pd.read_csv(file_path, index_col=0)\n",
    "    data.columns = range(1, len(data.columns) + 1)\n",
    "    expression_sums = data.iloc[:,1:].sum(axis=1)\n",
    "\n",
    "    filtered_data = data[expression_sums<3000]\n",
    "    filtered_data = data[expression_sums>0]\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "class GeneExpressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=2, stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=2, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(32 * 2, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, sequence_length)\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def sliding_window(data, window_size):\n",
    "    features = []\n",
    "    targets = []\n",
    "    for i in range(data.shape[1]-window_size):\n",
    "        features.append(data[:,i:i+window_size])\n",
    "        targets.append(data[:,i+window_size])\n",
    "    return np.array(features), np.array(targets)\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "def prepare_data(data, window_size, train_len):\n",
    "    all_data = data.values[:,1:train_len+1]\n",
    "    X, y = sliding_window(all_data, window_size)\n",
    "    X = X.astype(float).reshape(-1, window_size)\n",
    "    y = y.astype(float).reshape(-1, 1)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def create_dataloaders(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    X_train_tensor, y_train_tensor = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor, y_val_tensor = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = GeneExpressionDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = GeneExpressionDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    if X_test is not None and y_test is not None:\n",
    "        X_test_tensor, y_test_tensor = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "        test_dataset = GeneExpressionDataset(X_test_tensor, y_test_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        return train_loader, val_loader, test_loader\n",
    "    else:\n",
    "        return train_loader, val_loader\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, criterion, optimizer, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in test_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            total_loss += loss.item()\n",
    "    average_test_loss = total_loss / len(test_loader)\n",
    "    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n",
    "\n",
    "def get_prediction(input_data, model, device):\n",
    "    input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "    return prediction.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "def generate_prediction(predict_nums):\n",
    "    col_numbers = list(range(1, train_len + 1))\n",
    "    all_data = data.values[:,:train_len]\n",
    "    input_data = all_data[:,train_len-window_size:train_len]\n",
    "    input_data = input_data.astype(np.float32)\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(predict_nums):\n",
    "        pred = get_prediction(input_data, model, device)\n",
    "        predictions.append(pred)\n",
    "        input_data = np.concatenate((input_data[:, 1:], pred), axis=1)\n",
    "\n",
    "    all_predictions = np.concatenate(predictions, axis=1)\n",
    "\n",
    "    extended_data = np.concatenate((all_data, all_predictions), axis=1)\n",
    "\n",
    "    last_col_number = train_len\n",
    "    predicted_cols = [\"predicted_\" + str(last_col_number + i) for i in range(1, predict_nums + 1)]\n",
    "    columns = [str(num) for num in col_numbers] + predicted_cols\n",
    "\n",
    "    extended_df = pd.DataFrame(extended_data, index=data.index, columns=columns)\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fca201e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Training Loss: 7.1016, Validation Loss: 8.9654\n",
      "Epoch [2/40], Training Loss: 6.7450, Validation Loss: 8.2399\n",
      "Epoch [3/40], Training Loss: 7.1498, Validation Loss: 8.1713\n",
      "Epoch [4/40], Training Loss: 8.8702, Validation Loss: 9.9088\n",
      "Epoch [5/40], Training Loss: 7.1076, Validation Loss: 8.0427\n",
      "Epoch [6/40], Training Loss: 6.7078, Validation Loss: 7.9278\n",
      "Epoch [7/40], Training Loss: 6.4937, Validation Loss: 7.9550\n",
      "Epoch [8/40], Training Loss: 6.4882, Validation Loss: 8.2801\n",
      "Epoch [9/40], Training Loss: 6.6801, Validation Loss: 7.9369\n",
      "Epoch [10/40], Training Loss: 6.3999, Validation Loss: 7.9668\n",
      "Epoch [11/40], Training Loss: 6.4378, Validation Loss: 7.9420\n",
      "Epoch [12/40], Training Loss: 8.6744, Validation Loss: 9.5872\n",
      "Epoch [13/40], Training Loss: 6.9954, Validation Loss: 8.0626\n",
      "Epoch [14/40], Training Loss: 6.5237, Validation Loss: 7.9037\n",
      "Epoch [15/40], Training Loss: 6.4539, Validation Loss: 7.9047\n",
      "Epoch [16/40], Training Loss: 6.3987, Validation Loss: 7.9449\n",
      "Epoch [17/40], Training Loss: 6.5074, Validation Loss: 7.8941\n",
      "Epoch [18/40], Training Loss: 6.6366, Validation Loss: 7.8834\n",
      "Epoch [19/40], Training Loss: 6.2544, Validation Loss: 8.0670\n",
      "Epoch [20/40], Training Loss: 6.2436, Validation Loss: 8.0255\n",
      "Epoch [21/40], Training Loss: 6.1468, Validation Loss: 8.1341\n",
      "Epoch [22/40], Training Loss: 6.5344, Validation Loss: 7.8928\n",
      "Epoch [23/40], Training Loss: 6.5602, Validation Loss: 7.8701\n",
      "Epoch [24/40], Training Loss: 6.4999, Validation Loss: 7.8833\n",
      "Epoch [25/40], Training Loss: 6.3304, Validation Loss: 7.9381\n",
      "Epoch [26/40], Training Loss: 6.7107, Validation Loss: 7.9405\n",
      "Epoch [27/40], Training Loss: 6.3817, Validation Loss: 7.9093\n",
      "Epoch [28/40], Training Loss: 6.8652, Validation Loss: 8.0271\n",
      "Epoch [29/40], Training Loss: 6.1926, Validation Loss: 8.0284\n",
      "Epoch [30/40], Training Loss: 6.3921, Validation Loss: 7.8992\n",
      "Epoch [31/40], Training Loss: 6.3784, Validation Loss: 7.8487\n",
      "Epoch [32/40], Training Loss: 6.6487, Validation Loss: 7.8848\n",
      "Epoch [33/40], Training Loss: 6.8025, Validation Loss: 7.9591\n",
      "Epoch [34/40], Training Loss: 6.6027, Validation Loss: 7.8637\n",
      "Epoch [35/40], Training Loss: 8.1806, Validation Loss: 9.1167\n",
      "Epoch [36/40], Training Loss: 6.3038, Validation Loss: 7.9006\n",
      "Epoch [37/40], Training Loss: 6.1604, Validation Loss: 7.9473\n",
      "Epoch [38/40], Training Loss: 7.6593, Validation Loss: 8.5521\n",
      "Epoch [39/40], Training Loss: 6.2739, Validation Loss: 7.8809\n",
      "Epoch [40/40], Training Loss: 6.2613, Validation Loss: 7.8889\n",
      "Average Test Loss: 8.7466\n"
     ]
    }
   ],
   "source": [
    "def pipeline(filename, pred_len)\n",
    "    data = load_preprocess_data(filename)\n",
    "    train_len = 5\n",
    "    window_Size = 3\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(data, window_size, train_len)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNN1D().to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    train_model(model, train_loader, val_loader, num_epochs=40, criterion=criterion, optimizer=optimizer, device=device)\n",
    "    test_model(model, test_loader, criterion=criterion, device=device)\n",
    "    results = generate_prediction(pred_len)\n",
    "    results.to_csv('CNN_FB_PRED.csv')\n",
    "    \n",
    "pipeline('fbfiltered.csv', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "76a47b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_prediction(2)\n",
    "results.to_csv('CNN_FB_PRED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c6481a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Training Loss: 6.7417, Validation Loss: 8.0485\n",
      "Epoch [2/40], Training Loss: 6.8809, Validation Loss: 8.5778\n",
      "Epoch [3/40], Training Loss: 7.2232, Validation Loss: 8.5589\n",
      "Epoch [4/40], Training Loss: 6.6979, Validation Loss: 7.9980\n",
      "Epoch [5/40], Training Loss: 6.1280, Validation Loss: 8.1796\n",
      "Epoch [6/40], Training Loss: 6.4408, Validation Loss: 7.9840\n",
      "Epoch [7/40], Training Loss: 6.2594, Validation Loss: 8.0282\n",
      "Epoch [8/40], Training Loss: 6.7607, Validation Loss: 7.9993\n",
      "Epoch [9/40], Training Loss: 6.4251, Validation Loss: 8.2287\n",
      "Epoch [10/40], Training Loss: 6.1788, Validation Loss: 7.9834\n",
      "Epoch [11/40], Training Loss: 6.9092, Validation Loss: 7.8758\n",
      "Epoch [12/40], Training Loss: 6.2435, Validation Loss: 7.9399\n",
      "Epoch [13/40], Training Loss: 6.4791, Validation Loss: 7.8440\n",
      "Epoch [14/40], Training Loss: 6.4375, Validation Loss: 7.8922\n",
      "Epoch [15/40], Training Loss: 6.8956, Validation Loss: 8.5093\n",
      "Epoch [16/40], Training Loss: 6.2433, Validation Loss: 7.8325\n",
      "Epoch [17/40], Training Loss: 6.2481, Validation Loss: 7.7721\n",
      "Epoch [18/40], Training Loss: 6.7331, Validation Loss: 7.8281\n",
      "Epoch [19/40], Training Loss: 6.4833, Validation Loss: 7.7823\n",
      "Epoch [20/40], Training Loss: 7.9630, Validation Loss: 8.6995\n",
      "Epoch [21/40], Training Loss: 6.3027, Validation Loss: 7.7078\n",
      "Epoch [22/40], Training Loss: 6.1863, Validation Loss: 7.7325\n",
      "Epoch [23/40], Training Loss: 6.4107, Validation Loss: 7.7431\n",
      "Epoch [24/40], Training Loss: 6.0595, Validation Loss: 7.7485\n",
      "Epoch [25/40], Training Loss: 5.8234, Validation Loss: 7.9741\n",
      "Epoch [26/40], Training Loss: 6.4102, Validation Loss: 7.7553\n",
      "Epoch [27/40], Training Loss: 5.7739, Validation Loss: 7.9513\n",
      "Epoch [28/40], Training Loss: 7.4565, Validation Loss: 8.2229\n",
      "Epoch [29/40], Training Loss: 6.2677, Validation Loss: 7.6567\n",
      "Epoch [30/40], Training Loss: 6.6178, Validation Loss: 7.7175\n",
      "Epoch [31/40], Training Loss: 6.1095, Validation Loss: 7.7074\n",
      "Epoch [32/40], Training Loss: 7.5533, Validation Loss: 8.4352\n",
      "Epoch [33/40], Training Loss: 6.2558, Validation Loss: 7.6661\n",
      "Epoch [34/40], Training Loss: 6.7749, Validation Loss: 8.4050\n",
      "Epoch [35/40], Training Loss: 8.2899, Validation Loss: 9.2401\n",
      "Epoch [36/40], Training Loss: 5.9473, Validation Loss: 7.7870\n",
      "Epoch [37/40], Training Loss: 6.0605, Validation Loss: 7.7157\n",
      "Epoch [38/40], Training Loss: 6.2119, Validation Loss: 7.6739\n",
      "Epoch [39/40], Training Loss: 5.9217, Validation Loss: 7.7893\n",
      "Epoch [40/40], Training Loss: 6.5503, Validation Loss: 7.6900\n",
      "Average Test Loss: 9.3480\n"
     ]
    }
   ],
   "source": [
    "data = load_preprocess_data('fbfiltered.csv')\n",
    "train_len = 5\n",
    "window_size = 3\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(data, window_size, train_len)\n",
    "train_loader, val_loader, test_loader = create_dataloaders(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP().to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "train_model(model, train_loader, val_loader, num_epochs=40, criterion=criterion, optimizer=optimizer, device=device)\n",
    "test_model(model, test_loader, criterion=criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4f56df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_prediction(2)\n",
    "results.to_csv('MLP_FB_PRED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e40f2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "data = load_preprocess_data('fbfiltered.csv')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(data, window_size, train_len)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "def get_prediction_ML(input_data, model):\n",
    "    prediction = model.predict(input_data)\n",
    "    return prediction.reshape(-1, 1)\n",
    "\n",
    "def generate_prediction_ML(predict_nums):\n",
    "    col_numbers = list(range(1, train_len + 1))\n",
    "    all_data = data.values[:,:train_len]\n",
    "    input_data = all_data[:,train_len-window_size:train_len]\n",
    "    input_data = input_data.astype(np.float32)\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(predict_nums):\n",
    "        pred = get_prediction(input_data, model)\n",
    "        predictions.append(pred)\n",
    "        input_data = np.concatenate((input_data[:, 1:], pred), axis=1)\n",
    "\n",
    "    all_predictions = np.concatenate(predictions, axis=1)\n",
    "\n",
    "    extended_data = np.concatenate((all_data, all_predictions), axis=1)\n",
    "\n",
    "    last_col_number = train_len\n",
    "    predicted_cols = [\"predicted_\" + str(last_col_number + i) for i in range(1, predict_nums + 1)]\n",
    "    columns = [str(num) for num in col_numbers] + predicted_cols\n",
    "\n",
    "    extended_df = pd.DataFrame(extended_data, index=data.index, columns=columns)\n",
    "    return extended_df\n",
    "\n",
    "results = generate_prediction_ML(2)\n",
    "results.to_csv('Linear_FB_PRED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f9d8eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "data = load_preprocess_data('fbfiltered.csv')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(data, window_size, train_len)\n",
    "\n",
    "model = SVR(C=1.0, kernel='rbf')\n",
    "model.fit(X_train, y_train.ravel())\n",
    "results = generate_prediction_ML(2)\n",
    "results.to_csv('SVR_FB_PRED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c580f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "data = load_preprocess_data('fbfiltered.csv')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(data, window_size, train_len)\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train, y_train.ravel())\n",
    "results = generate_prediction_ML(2)\n",
    "results.to_csv('RFR_FB_PRED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3d23009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "data = load_preprocess_data('fbfiltered.csv')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(data, window_size, train_len)\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "model.fit(X_train, y_train.ravel())\n",
    "results = generate_prediction_ML(2)\n",
    "results.to_csv('KNN_FB_PRED.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
